{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8cc949",
   "metadata": {},
   "source": [
    "### Review  / Questions [30 minutes]\n",
    "1. See Nov08 TUT and Nov11 LEC\n",
    "2. Review multiple linear regression and use the opportunity to introduce train test validation\n",
    "    1. (For the same data) R-squared must increase with each additional predictor variable added to the model\n",
    "    2. Adjusted R-squared attempts to \"correct\" this phenomenon based on a sort of \"theoretical expectation\" or \"rule of thumb\"\n",
    "    3. Prediction performance could also be judge by how good R-squared is for \"new\" data that the model\n",
    "        1. If R-squared reduces for \"new\" data the model wasn't trained is compared to \"data the model has seen\" there's a problem...\n",
    "        2. The *Train-Test* framework gives us a way to construct \"new\" data to test in this way\n",
    "            1. Watch out for variability in *Train-Test* scores across different random divisions of the data though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60dfcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c468b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = datasets.load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(data=cancer_data.data, columns = cancer_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0670a35c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'breast_cancer.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that we're not using 'target' which is binary\n",
    "# with 'target_names': array(['malignant', 'benign'], dtype='<U9')\n",
    "cancer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c29e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(cancer_data['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f6a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split the data into 80% training data and 20% testing data\n",
    "np.random.seed(131)\n",
    "training_indices = cancer_df.sample(frac=0.80, replace=False).index.sort_values()\n",
    "testing_indices = cancer_df.index[~cancer_df.index.isin(training_indices)]\n",
    "# now we'll use part of the data to \"train\" the MLR and \"test\" it with the other part of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab647fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7982160760289563"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The outcome variable is continuous (not binary) and so are the predictor variables\n",
    "MLR = smf.ols('Q(\"mean compactness\") ~ Q(\"mean radius\") + Q(\"mean concavity\")', \n",
    "              data=cancer_df.loc[training_indices,:])\n",
    "# Fit the mulitple lienar regression model (MLR)\n",
    "MLR_fit = MLR.fit()\n",
    "np.corrcoef(MLR_fit.predict(cancer_df.loc[training_indices,:]),\n",
    "            cancer_df.loc[training_indices,\"mean compactness\"])[0,1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c0b1421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7809867032981558"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"In sample\" performance based on the \"training data\" (above) \n",
    "# is different from \n",
    "# \"out of sample\" performance based on the \"testing data\" (below) \n",
    "np.corrcoef(MLR_fit.predict(cancer_df.loc[testing_indices,:]),\n",
    "            cancer_df.loc[testing_indices,\"mean compactness\"])[0,1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad60049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fact that the model fit as evaluated by R-squared (proportion of variation explained)\n",
    "# is worse for \"new\" data as opposed to \"data the models was trained on and has already seen\"\n",
    "# is generally what we'd expect to see in an analysis like this...\n",
    "\n",
    "# The good news is that the \"out of sample\" predictive (proportion of variation explained) performance \n",
    "# is not much worse than the \"in sample performance\" which we'd typically interpret as suggesting that\n",
    "# the model seems to represent the data fairly well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00b55b",
   "metadata": {},
   "source": [
    "### Demo [45 minutes] \n",
    "> Concept only, we will reserve coding for lecture \n",
    "1. Introduce the basic concept of a decision tree\n",
    "2. Explain the confusion matrix and what it means \n",
    "\n",
    "~3. Introduce concept of Sensitivity, Specificity, Accuracy, Precision~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8b51b",
   "metadata": {},
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230424141242/dcr.png\" alt=\"Optional Title\" style=\"width: 500px; height: auto;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24650092",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:667/1*3yGLac6F4mTENnj5dBNvNQ.jpeg\" alt=\"Optional Title\" style=\"width: 500px; height: auto;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73746029",
   "metadata": {},
   "source": [
    "##### ~Sensitivity (True Positive Rate)~\n",
    "~Sensitivity measures the proportion of actual positives that are correctly identified.\n",
    "$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$~\n",
    "\n",
    "##### ~Specificity (True Negative Rate)~\n",
    "~Specificity measures the proportion of actual negatives that are correctly identified.\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$~\n",
    "\n",
    "##### ~Accuracy~\n",
    "~Accuracy measures the proportion of true results (both true positives and true negatives) in the population.\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$~\n",
    "\n",
    "##### ~Precision (Positive Predictive Value)~\n",
    "~Precision measures the proportion of positive identifications that were actually correct.\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce69a7",
   "metadata": {},
   "source": [
    "#### Communication [so this is being worked on / revised I'd say]\n",
    "\n",
    "\n",
    "1. **[15 minutes]** Break into 5 groups of 5 and prepare a speech describing what is the purpose of train test verification\n",
    "   \n",
    "2. **[20 minutes]** Which parameter is the most important?  Sensitivity, Specificity, Accuracy, or Precision. Use GPT to answer this question and then discuss with group member about your conclusion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f3680",
   "metadata": {},
   "source": [
    "\n",
    "#### Homework  -- To be completed before lecture--\n",
    "\n",
    "> Code and write all your answers in a python notebook (in code and markdown cells) and save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking.\n",
    "\n",
    "1. Understanding Decision Trees\n",
    "    1. \"Explain Decision Trees: Write a brief paragraph explaining what a decision tree is. Include in your response a description of how decision trees are used in data analysis, the type of problems they can solve, and how they make decisions at each node. Provide an example of a real-world application where decision trees might be particularly useful.\"\n",
    "\n",
    "2. Importance of Model Evaluation Metrics\n",
    "    1. \"Evaluate the Importance of Different Model Evaluation Metrics: For each of the following metrics - Sensitivity, Specificity, Accuracy, and Precision - provide a scenario where focusing on that particular metric is crucial. Explain why that metric is important in your given scenario and how it influences the decision-making process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4893ab",
   "metadata": {},
   "source": [
    "#### Homework  -- To be completed after lecture--\n",
    "\n",
    "> Code and write all your answers in a python notebook (in code and markdown cells) and save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58bd64b",
   "metadata": {},
   "source": [
    "### Q0:  We begin by importing dataset and the libraries we will use. For each import write one comment briefly explaining the purpose of the import. Then play around with this data and provide some work demonstrating what the data is.\n",
    "#### Remember if you don't know the answer, you can always use ChatGPT or other resources to help you figure things out. Just make sure you understand your answers by the time you submit them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4b50624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, model_selection\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import graphviz as gv\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "ab = pd.read_csv(\"amazonbooks.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed4301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0444816",
   "metadata": {},
   "source": [
    "### Q1: Create a sequence of new dataframes `ab_reduced`, `ab_reduced_noNaN`, `ab_reduced_noNaN_train`, and `ab_reduced_noNaN_test` according to the following specifications.\n",
    "\n",
    "\n",
    "1. `Weight_oz`, `Width`, and `Height` should be removed because they have a lot of missing values that would cause us to lose rows if they were kept in the analysis; but, we're going to be okay not using them for the analysis. \n",
    "2. All rows with `NaN` entries should be dropped.\n",
    "3. `Pub year` and `NumPages` should be redefined to have the type `int`, and `Hard_or_Paper` should be redefined to have the type `category`.\n",
    "4. Create an 80/20 split with 80% training set and 20% testing set.\n",
    "\n",
    "#### To complete the final 80/20 split in a reproducible way, set a \"random seed\".\n",
    "\n",
    "> Only remove rows with `NaN` entries once you've subset to the columns you're interested in. This will minimize potentially unnecessary data loss... Of course we might want to consider imputing missing data to further mitigate data loss, but the considerations for doing so are more advanced than the level of our course, so we'll not consider that for now. At any rate, `NaN` entries can't be used in their raw form with the `scikit-learn` methodologies below, so we do need to remove them to proceed with our analyses.\n",
    "\n",
    "#### Remember if you don't know how to do something , you can always use ChatGPT or other resources to help you figure out what you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54433c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9906b2d",
   "metadata": {},
   "source": [
    "### Q2: Tell ChatGPT that you are about to fit a \"scikit-learn\" `DecisionTreeClassifier` model and ask what the following preparation steps are doing. One you understand the answer, write a one to two sentence summary explaining these preparation steps in your own words (in the markdown cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "835c4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00028705",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eed1122",
   "metadata": {},
   "source": [
    "### Q3: Train a classification tree `clf` using only the  `List Price` variable to predict whether or not a book is hard cover book or a paper cover book (with  `max_depth` parameter set to `2`) .\n",
    "\n",
    "#### Use default values for all (tuning) parameters instantiating the Decision Tree Classifier.\n",
    "\n",
    "> - Hint 1: in the end you'll need to use you'll need to use `DecisionTreeClassifier(...).fit(...)` where the first `...` represents the details of instantiating a specific version of the `DecisionTreeClassifier` and the second `...` represents the parameters of the `.fit(...)` of the `DecisionTreeClassifier(...)` object\n",
    "method actually... asking ChatGPT about \"`DecisionTreeClassifier .fit(...)\" can be helpful here; and, for a deeper and more acurate explaination you could find the `scikit-learn` \"Decision Trees\" documentation online...\n",
    "> - Hint 2: should you use the `ab_reduced_noNaN` data, or the `ab_reduced_noNaN_train` data, or the `ab_reduced_noNaN_test` data to initially fit the classification tree? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18aeaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6afe95b",
   "metadata": {},
   "source": [
    "Visualize your decision tree using the `tree.plot_tree(clf)` function shown in the `sklearn` documentation [here](\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#what-is-the-values-array-used-here) and [here](https://scikit-learn.org/stable/modules/tree.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff716801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36c2d5e0",
   "metadata": {},
   "source": [
    "Visualize your decision tree again, but this time make it more immediately readible using `graphviz`, which is demonstrated in the `sklearn` documentation [here](https://scikit-learn.org/stable/modules/tree.html#alternative-ways-to-export-trees) **once you've clicked the \"Alternative ways to export trees\" button**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b5f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f664d469",
   "metadata": {},
   "source": [
    "And to limit the visualization itself we can add the `max_depth` parameter to our call of `export_graphviz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a857671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c225c340",
   "metadata": {},
   "source": [
    "### Q4: How many observation are in the training data set and the test data set?\n",
    "\n",
    "> - Hint: a single observation consists of all the measurements made on a single entity. In Machine Learning, the  \"vector\" of all values measured for a single entity comprise a single \"observation\" so this just corresponds (typically) to a row of a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96a836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98b3ce1f",
   "metadata": {},
   "source": [
    "### Q5: Assuming you correctly fit the classification tree with the `ab_reduced_noNaN_train` data as opposed to the `ab_reduced_noNaN` or the `ab_reduced_noNaN_test` data sets, why did you do this?\n",
    "\n",
    "#### Write a one to two sentence answer to this question in markdown cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce0805",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dde2427",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "890b5097",
   "metadata": {},
   "source": [
    "### Q6: Train classification tree `clf2` with features `NumPages`, `Thick` and `List Price` (to again predict if a book has hardback or paperback), use `GridSearchCV` to find best `max_depth`.\n",
    "\n",
    "#### Use the same train/test split data used so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfade813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d889386a",
   "metadata": {},
   "source": [
    "### Q7: Use the testing dataset you created in Q1 to create confusion matrices for `clf` and `clf2`. Report the sensitivity (true positive rate), specificity (true negative rate) and accuracy for each of the trees/models.\n",
    "\n",
    "#### Provide your answers as decimal numbers with three signifiant digits, such as `0.123` (and not as percentages like `12.3%`), and treat “Good” life expectancy as the positive response and prediction class. \n",
    "\n",
    "> Here are few things that might be helpful to ask ChatGPT: \n",
    "> - Hint 0: How can I use `np.round()`   \n",
    "> - Hint 1: Does the `y_true` or `y_pred` parameter go first in the `confusion_matrix` function?  \n",
    "> - Hint 2: How to label a confusion matrix in `sklearn`\n",
    "> - Hint 3: Confusion Matrices and Metrics explainations just below, you can always ask Chat GPT to explain it in detail or give you some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f395db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12218df9",
   "metadata": {},
   "source": [
    "<a id='cf'></a>\n",
    "# Confusion Matrices and Metrics\n",
    "\n",
    "- **Accuracy** is the proportion of cases that are correctly identified.\n",
    "- **Sensitivity** is the proportion of actual positive cases which are correctly identified to be positive (as true positives)\n",
    "    - **Sensitivity** is also known as **true positive rate (TPR)**\n",
    "- **Specificity** is the proportion of actual negative cases which are correctly identified to be negative (as true negative)\n",
    "    - **Specificity** is also known as **true negative rate (TNR)**\n",
    "- **False positive rates (FPR)** are defined to be the proportion of actually negative cases which are incorrectly identified (as false positives)\n",
    "- **False negative rates (FNR)** are defined to be the proportion of actually positive cases which are incorrectly identified (as false negatives)\n",
    "    - *but noticed how the FPR and FNR work in a sort of \"flipped\" manner in these definitions as they are defined with respect to the truth*\n",
    "\n",
    "In formulas\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy}  & = {} (TP+TN)/\\text{\"total # of cases\"}\\\\\n",
    "TPR & = {} TP/(TP+FN) = 1-FNR \\\\\n",
    "TNR & = {} TN/(TN+FP) = 1-FPR\n",
    "\\end{align*}\n",
    "\n",
    "and you can read more and see a (greatly expanded) handy list of formulas at the following [wikipedia page.](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72576c25",
   "metadata": {},
   "source": [
    "### Q8: Explain what is causing the differences between the following two confusion matrices below, and why the two confusion matrices above (for `clf2` and `clf3`) are better\n",
    "\n",
    "#### Write a three to four sentence answer to this question in markdown cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb30e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(ab_reduced_noNaN_train.life_exp_good, \n",
    "                                        clf.predict(ab_reduced_noNaN_train[['List Price']]), labels=[0, 1]), \n",
    "                       display_labels=[\"Paper\",\"Hard\"]).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix(ab_reduced_noNaN_test.life_exp_good, \n",
    "                                        clf.predict(ab_reduced_noNaN_testtest[['List Price']]), labels=[0, 1]), \n",
    "                       display_labels=[\"Paper\",\"Hard\"]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4eae0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "526939c2",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "Compared to understanding the contribution of different covariates towards the final predicted values of multiple linear regression models (where you can just read off the equation to see how predictions work), the extent to which we do not understand the overall contributions of the different features to the final predictions from our decision trees should feel a bit off-putting. To remedy this we can use so-called **Feature Importance** heuristics to judge how relatively important the different features are in the final decision tree predictions. \n",
    "\n",
    "\n",
    "### Q9: Ask ChatGPT or search on internet how to check feature Importance in `scikit-learn`, read the following paragraph and make sure you understand it \n",
    "\n",
    "> The way a decision tree is fit is that at each step in the construction process of adding a new decision node splitting rule to the current tree structure, all possible decision rules for all possible variables are considered, and the one that improves the prediction the most (as measured by the criterion of either \"Gini impurity\" or \"Shannon entropy\") legally and sufficiently according to the tuning parameters rules of the decision tree is added to the decision tree.  The overall \"criterion\" noted above improves with each new decision node splitting rule, so the improvement can thus be tracked and the contributions attributed to the feature upon which the decision node splitting rule is based.  This means the relative contribution of each feature to the overall explanatory power of the model can be calculated, and this is what the `.feature_importances_` attribute does. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dd793",
   "metadata": {},
   "source": [
    "### Q10: Which predictor variable is most important for making predictions according to `clf2`?\n",
    "\n",
    "#### Visualize the *Feature Importances* and report the name of most important feature and its numeric *Feature Importance* value\n",
    "\n",
    "> - Hint 0: `.feature_importances_` &`.feature_names_in_` \n",
    "> - Hint 1: Visualize Feature Importances : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fab485",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10_most_important_feature = \n",
    "Q10_most_important_feature_percentage_score = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7d92c",
   "metadata": {},
   "source": [
    "### Q11: Describe the differences of interpreting coefficients in linear model regression versus feature importances in decision trees.\n",
    "\n",
    "#### Write a couple sentences or so in markdown cell below to answer this question \n",
    "\n",
    "> Hint: linear model regression predicts continuous real-valued averages for a given configuration of covariate values (or, feature values, if we're using machine learning terminology instead of statistical terminology), whereas a binary classification model such as a binary classification tree predicts 0/1 (\"yes\" or \"no\") outcomes (and gives the probability of a 1 \"yes\" (or \"success\") outcome from which a 1/0 \"yes\"/\"no\" prediction can be made; but, this is not what is being asked here. This question is asking \"what's the difference in the way we can interpret and understand how the predictor variables influence the predictions in linear model regression based on the coefficients versus in binary decision trees based on the Feature Importances?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366156c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment questions would go here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
