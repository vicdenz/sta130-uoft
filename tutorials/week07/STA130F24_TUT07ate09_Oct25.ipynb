{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d66e0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA130 TUT 7ate9 (Oct25)<br><br>üìà‚ùì <u>Simple Linear Regression</u><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Model Fitting / Hypothesis Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd7d29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ‚ôªÔ∏è üìö Review / Questions [10 minutes]\n",
    "\n",
    "### 1. Follow up questions and clarifications regarding the ideas of **correlation** and the \"straight line association\" model of **Simple Linear Regression** from the Oct21 LEC<br>\n",
    "\n",
    "<details class=\"details-example\"><summary><u><span style=\"color:blue\">Simple Linear Regression Terminology</span></u> (reference for <b>Communication Activity #1 question 2</b> below)</summary>\n",
    "\n",
    "$$ \\Large Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$\n",
    "\n",
    "- **Outcome** $Y_i$ is a **continuous numeric variable**\n",
    "\n",
    "> **Outcome** $Y_i$ can also be called a **response**, **dependent**, or **endogenous variable** in some domains and contexts\n",
    "\n",
    "- **Predictor variable** $x_i$ is a **numeric variable**\n",
    "\n",
    "> - Fow now we'll consider $x_i$ to be a **continuous** numeric variable, but this is not necessary, and we will consider versions of $x_i$ later\n",
    "> - **Predictor variable** $x_i$ can also be called an **explanatory**, **independent**, or **exogenous variable**, or a **covariate** or **feature** (which are the preferred terms in the statistics and machine learning domains, respectively)\n",
    "\n",
    "- **Intercept** $\\beta_0$ and **slope** $\\beta_1$ are the two primary **parameters** of a **Simple Linear Regression** model\n",
    "\n",
    "> **Intercept** and **slope** describe a **linear** (\"straigh line\") relationship between **outcome** $Y_i$ and **predictor variable** $x_i$\n",
    "\n",
    "- **Error** $\\epsilon_i$ (also sometimes called the **noise**) makes **Simple Linear Regression** a **statistical model** by introducing a **random variable** with a **distribution**\n",
    "\n",
    "- The $\\sigma^2$ **parameter** is a part of the **noise distribution** and controls how much vertical variability/spread there is in the $Y_i$ data off of the line: $\\sigma^2$ is an \"auxiliary\" **parameter** in the sense that interest is usually in $\\beta_0$ and $\\beta_1$ rather than $\\sigma^2$\n",
    "\n",
    "> - **Errors** $\\epsilon_i$ (in conjuction with the **linear form**) define the **assumptions** of the **Simple Linear regression** Model specification\n",
    "> - <u>but these **assumptions** are not the focus of further detailed reviewed here</u>\n",
    "\n",
    "</details>    \n",
    "    \n",
    "<details class=\"details-example\"><summary><u><span style=\"color:blue\">Further details regarding the assumptions</span></u> (which <b>should not the focus of further detailed reviewed here</b>)</summary>\n",
    "\n",
    "> The first three assumptions associated with the **Simple Linear regression** model are that<br><br>\n",
    "> \n",
    "> 1. the $\\epsilon_i$ **errors** (sometimes referred to as the **noise**) are **normally distributed**\n",
    "> 2. the $\\epsilon_i$ **errors** are **homoscedastic** (so their distributional variance $\\sigma^2$ does not change as a function of $x_i$)\n",
    "> 3. the linear form is [at least reasonably approximately] \"true\" (in the sense that the above two remain [at least reasonably approximately] \"true\") so that then behavior of the $Y_i$ **outcomes** are represented/determined on average by the **linear equation**)<br>\n",
    ">\n",
    ">    and there are additional assumptions; but, a deeper reflection on these is \"beyond the scope\" of STA130; nonetheless, they are that<br><br>\n",
    "> 4. the $x_i$ **predictor variable** is **measured without error**\n",
    "> 5. and the $\\epsilon_i$ **errors** are **statistically independent** (so their values do not depend on each other)\n",
    "> 6. and the $\\epsilon_i$ **errors** are **unbiased** relative to the **expected value** of **outcome** $E[Y_i|x_i]=\\beta_0 + \\beta_1x_i$ (which is equivalently stated by saying that the mean of the **error distribution** is $0$, or again equivalently, that the **expected value** of the **errors** $E[\\epsilon_i] = 0$)\n",
    "\n",
    "</details><br>  \n",
    "    \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> This TUT will introduce **Hypothesis Testing** in the **Simple Linear Regression** context for the purposes of evaluating a **null hypothesis** assumption of \"no association\" between two numeric variables $Y$ and $x$ relative to an **alternative hypothesis** of \"straight line association\" (meaning that changes in the $x$ variable have corresponding changes in the $Y$ variable \"on average\")\n",
    "    \n",
    "</details>\n",
    "\n",
    "### 2. Follow up questions and clarifications regarding concepts associated with the **sampling distribution** topic <u>*[REALLY needs to be addressed in OH at this point]*<br><br> HW this time is Going To Be DIFFERENT: you MUST understand simulation to do it</u>\n",
    "\n",
    "> AKA **Hypothesis Testing**, **Sampling Distribution under the Null Hypothesis**, and related topics regarding interpretation from Oct04 TUT and Oct11 TUT; AND, **Sampling Distribution**, **Bootstrapped Confidence Intervals**, and related topics regarding interpretation from Sep27 TUT and Sep30 LEC and Oct07 LEC \n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> Understanding the fundamental underlying mechanism of a **sampling distribution** (most easily demonstrated through **simulation**) is necessary for creating a deep understanding of its (and *the*) two primary applications in **statistics**: **Hypothesis Testing** and **[Bootstrapped] Confidence Intervals**\n",
    "> \n",
    "> Further clear understanding regarding the abstract components and process of **Hypothesis Testing** (decision making regarding parameters using null hypotheses and p-values) is additionally needed from here, as this serves as an unavoidably necessary pre-requesite foundation upon which the enevitable extension of **Hypothesis Testing** to more advanced analyses (such **Multiple Linear Regression** and **permutation testing**, and **Simple Linear Regression** which will be the focus of this TUT) are based\n",
    "    \n",
    "</details><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969dd2c",
   "metadata": {},
   "source": [
    "## üí¨ üó£Ô∏è Communication Activity #1 [20 minutes]\n",
    "\n",
    "To the best of your abilty, recreate the <u>**FIVE**</u> groups from the **Communication Acivity** of the previous (Oct04 and Oct11) TUTs <br><br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Stella McStat's Wheel of Destiny</u></summary>\n",
    "\n",
    "We should all by now hopefully be VERY familiar with this by this point in time given that this was a focus of the Oct04 and Oct11 TUTs **and was heavily featured on the midterm exam**...\n",
    "    \n",
    "### The Wheel of Destiny\n",
    "\n",
    "Stella McStat had been running a small-time gambling operation on campus for several months during her first year at UofT... \n",
    "\n",
    "- For each spin of the wheel, two gamblers take part. For a toonie each (\\\\$2 Canadian), Stella sells one a red ticket and one a black ticket  (i.e., total \\\\$4). Then Stella spins the Wheel of Destiny. The person who holds the colour on which the spinner stops gets \\\\$3.50 (Stella keeps \\\\$0.50 per spin for running the game and providing snacks).\n",
    "\n",
    "Stella just bought a new spinner, the critical piece of equipment for this game. She's heard some mixed reviews about the manufacturer she has purchased from. Before she beings using this spinner, she wants to make sure that it is, in fact, fair (meaning, she wants both colours to come up equally often). Because of the set-up of the game, Stella has no incentive to cheat and wants the game to be as fair as possible.\n",
    "\n",
    "Everything phystical and mechanical that Stella can examine about the wheel seems fine; there is the same number of sectors of each colour and they each have the same area. BUT! Stella has a great idea and decides to come to YOU, her statistical guru, and ask you to verify that the new spinner is fit to use. Is Stella's game is \"fair\" (even if somewhat illegal)?\n",
    "\n",
    "| <img src=\"https://i.postimg.cc/BvqJwBwc/stella2.png\" style=\"height: 450px;\"/> |  <img src=\"https://i.postimg.cc/vm3GRxJR/fair.png\" style=\"height: 450px;\"/> |\n",
    "|-|-|\n",
    "|An Exercise for Illustrating the Logic of Hypothesis Testing|Adapted from Lawton, L. (2009), Journal of Stat. Education, 17(2)|\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4816de6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discuss the following\n",
    "\n",
    "1. **[8 of the 20 minutes]** What is the **Null** (and **Alternative**) **Hypothesis** and what is the definition of (and using **simulation** how do you estimate) a **p-value**? \n",
    "\n",
    "> First answer this question specifically for the context of \"Stella McStat's Wheel of Destiny\", but then see if you can give an answer that is more abstract and to some degree \"context free\" (in terms of **parameters** and [observed versus simulated] **statistics**)\n",
    "\n",
    "2. **[12 of the 20 minutes]** Examine the theoretical **Simple Linear Regression** model below and consider what a **Null** (and **Alternative**) **Hypothesis** and **p-value** could be for this context? \n",
    "\n",
    "$$\\Large Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma\\right)$$\n",
    "\n",
    "> **Hints**\n",
    "> \n",
    "> 1. What is the data and how many data points are there?\n",
    ">  \n",
    ">  \n",
    "> 2. What is $\\epsilon_i$ and how many of them are there?\n",
    ">  \n",
    ">  \n",
    "> 3. What values can the **slope** $\\beta_0$ and **intercept** $\\beta_1$ can have?  \n",
    ">  \n",
    "> \n",
    "> 4. Are **Null** and **Alternative Hypotheses** about **samples** like $x_i$ or $Y_i$ or **sample stastics** like $\\bar Y$ or $\\bar x$, or **population parameters** like $\\mu$?\n",
    ">  \n",
    ">  \n",
    "> 5. There's not a **Null** and **Alternative Hypotheses** regarding $\\epsilon_i$, but there are plenty of assumptions about it (which technically are *a part* of the **Null hypothesis**)... what are those assumptions about $\\epsilon_i$? \n",
    ">  \n",
    ">  \n",
    "> 6. Do you have any intuition of how to think about the conceptual meaning of a **p-value** (defined as \"the probability that a test statistic is as or more extreme than the observed test statistic if the null hypothesis is true\") in terms of **simulation** in the context of **Simple Linear Regression**?\n",
    ">  \n",
    ">  \n",
    "> To be discussed in more detail shortly, the **fitted model** $\\hat y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i$ corresponding to the theoretical model above is based on observed **sample data**. So, e.g., the **fitted slope** $\\hat \\beta_1$ is a **statistic** (which has a **sampling distribution**) that corresponds to the theoretical **slope** parameter $\\beta_1$...\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bfe9a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo (of Model Fitting and Hypothesis Testing for the Simple Linear Regression Model)  [45 minutes]\n",
    "\n",
    "### Terminology [12 of the 45 minutes]\n",
    "\n",
    "$$\\LARGE \\text{Based on data we get} \\quad \\hat y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i \\quad \\text{from}$$\n",
    "\n",
    "$$\\LARGE Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma\\right)$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083ba3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The $\\hat y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i$ **fitted model** equation distinctly contrasts with the $Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$ **theoretical model** specification. To emphasize and clarify the difference, we augment our **simple linear regression** model nomenclature (as given in the \"**Review / Questions**\" section above) with the contrasting alternative notations and terminology: \n",
    "\n",
    "- **Fitted intercept** $\\hat \\beta_0$ and **slope** $\\hat \\beta_1$ ***coefficients*** are given \"hats\" to distinguish that they **estimate** (based on observed **sample data**), respectively, the **intercept** $\\beta_0$ and **slope** $\\beta_1$ ***parameters***<br><br>\n",
    "\n",
    "- **Fitted (predicted) values** $\\hat y_i$ are made lower case and also given \"hats\" to distinguish them from the (upper case) **theoretical random variable** $Y_i$ implied by the **theoretical simple linear regression model**\n",
    "  \n",
    "> Technically, the **error** $\\epsilon_i$ is the **random variable** specified by the **simple linear regression model** specification, and this implies the **random variable** nature of $Y_i$ \n",
    "\n",
    "- The **residuals** $\\text{e}_i = \\hat \\epsilon_i = y_i - \\hat y_i = y_i - \\hat \\beta_0 + \\hat \\beta_1 x_i $ also distinctly contrast with the **errors** (or **noises**) $\\epsilon_i$\n",
    "    \n",
    "> The **residuals** $\\text{e}_i = \\hat \\epsilon_i$ are actually available, while the **error** (or **noises**) $\\epsilon_i$ are just a theoretical concept\n",
    "> \n",
    "> The **residuals** $\\text{e}_i = \\hat \\epsilon_i$ nonetheless are therefore used to diagnostically assess the theoretical modeling assumptions of the  **errors** $\\epsilon_i$, such as the **normality**, **homoskedasticity**, and **linear form** assumptions; and, <u>while this is a not necessarily beyond the scope of STA130 and would certainly be a relevant consideration for the course project, this will not be addressed here at this time</u>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad5445",
   "metadata": {},
   "source": [
    "### Observed Data Setup [5 of the 45 minutes]\n",
    "\n",
    "Imagine you noticed that the prices of shuttlecocks in the nearby store have increased. At the same time, suppose you are also aware that there has been a recent surge in bird flu cases. You suddenly wonder if there might be a connection between these two events. So you get some historical data as given in the format below.\n",
    "\n",
    "|Bird Flu Cases |Shuttlecock Price ($)|\n",
    "|:-------------:|:----------------------------:|\n",
    "|1000           |3.0                           |\n",
    "|1522           |4.2                           |\n",
    "|$$\\vdots$$     |$$\\vdots$$                    |\n",
    "|1200           |3.2                           |\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> Actually, this data and all the related (analysis and plotting) code was made by just giving instructions to a ChatBot, and tweaking things a little bit. As long as you know what to ask for and what you're looking for a ChatBot can carry out **Simple Linear Regression** and related tasks (and follow any specific analyses adjustment directions you  request).\n",
    "\n",
    "</details>\n",
    "\n",
    "#### In the visual representation of the data below, what are we considering the (dependent) outcome $Y_i$ and what are we considering the (independent) predictor $x_i$? Does this seem sensible given the framing of our inquiry? \n",
    "\n",
    "|<img src=\"https://www.mumbailive.com/images/news/bird-flu1_151660804012.jpg?w=1368\" alt=\"Bird Flu\" style=\"width: 500px; height: 300px;\"/>|<img src=\"https://www.badmintonskills.net/wp-content/uploads/2015/09/Badminton-004.jpg?x83573\" alt=\"Shuttlecock\" style=\"width: 250px; height: 300px;\"/>|\n",
    "|-:|:-|\n",
    "|Assess a possible association between bird flu prevalence and the price of shuttlecocks| using **Simple Linear Regression**|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71bb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Here's the data\n",
    "data = {\n",
    "    'Bird Flu Cases': [1000, 1522, 1300, \n",
    "        1450, 1550, 1350, 1250, 1500, 1150, 1650, 1300, 1400, 1750, \n",
    "        1800, 900, 1100, 1700, 1400, 1600, 1200],\n",
    "    'Shuttlecock Price': [3.0, 4.3, 3.7, \n",
    "        3.9, 4.0, 3.5, 3.4, 4.0, 3.2, 4.4, 3.6, 3.7, 4.6, \n",
    "        4.9, 2.8, 3.1, 4.6, 3.9, 4.4, 3.2]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Here's the data visually\n",
    "fig = px.scatter(df, x='Bird Flu Cases',  y='Shuttlecock Price', \n",
    "                 title='Shuttlecock Price vs. Bird Flu Cases',\n",
    "                 labels={'Bird Flu Cases': 'Bird Flu Cases',\n",
    "                         'Shuttlecock Price': 'Shuttlecock Price ($)'})\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/52771328/plotly-chart-not-showing-in-jupyter-notebook\n",
    "import plotly.offline as pyo\n",
    "# Set notebook mode to work in offline\n",
    "pyo.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e11268",
   "metadata": {},
   "source": [
    "### *Model Fitting* via `statsmodels.formula.api as smf` and `smf.ols(...).fit()`<br>[16 of the 45 minutes]\n",
    "\n",
    "- First we'll demonstrate getting a fitted Simple Linear Regression Model using `statsmodels`  and working with the key elements of the fitted model [10 of these 15 minutes]  \n",
    "\n",
    "\n",
    "- Then we'll visually demonstrate the fitted Simple Linear Regression model based on its estimated intercept and slope parameters [5 of these 15 minutes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d712088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# And here's how to do \"Model Fitting\" for Simple Linear Regression with `statsmodels`\n",
    "\n",
    "# Use \"Y~x\" R-style formulas: https://www.statsmodels.org/stable/example_formulas.html\n",
    "linear_specification = 'Q(\"Shuttlecock Price\") ~ Q(\"Bird Flu Cases\")'\n",
    "# The notation above is admittidly starnge, but it's because \n",
    "# there are spaces in my column (variable) names in the data\n",
    "\n",
    "# Put the data into the a `statsmodels` \"model\" object\n",
    "model_data_specification = smf.ols(linear_specification, data=df)\n",
    "\n",
    "# Fit the model\n",
    "fitted_model = model_data_specification.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9eb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the results...\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ab27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's too much in the full `.summary()`: just focus on this table for now\n",
    "fitted_model.summary().tables[1] # does this indexing make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1ac63",
   "metadata": {},
   "source": [
    "$$\\LARGE \\text{For the data above} \\quad \\hat y_i = 0.5361+0.0023 \\times x_i$$\n",
    "\n",
    "$$\\LARGE \\text{is the} \\quad \\hat y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i \\quad \\text{estimating the model}$$\n",
    "\n",
    "$$\\LARGE Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma\\right)$$\n",
    "\n",
    "#### What are $\\hat \\beta_0$ and $\\hat \\beta_1$? What are $Y_i, x_i$, and $\\hat y_i$ and $\\text{e}_i = \\hat \\epsilon_i$? How do you interpret the fitted model?<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> When we create a **fitted Simple Linear Regression model** for an observed data dataset, we obtain **estimates** of what the data suggests the **intercept** and **slope** could be to form an equation of the (predicted) values for the data\n",
    "> \n",
    "> Model fitting is typically based on the \"ordinary least squares\" (`ols`) concept (maximizing **R-squared**), although there are analytical closed form solutions for the **intercept** **slope coefficient estimates** for **Simple Linear Regression models**; but, these considerations will not be detailed further here (as they'll be the focus of a HW question) and <u>we'll now instead just focus on fitting models using the `python` `statsmodels` package</u>\n",
    "    \n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[\"Shuttlecock Price\"]\n",
    "x = df[\"Bird Flu Cases\"]\n",
    "\n",
    "# The printout coeficient values are rounded\n",
    "# pd.DataFrame({\"formula\": 0.4291+0.0024*x,\n",
    "#               \"model\": fitted_model.fittedvalues})\n",
    "\n",
    "# So we use the exact fitted coefficient values using `fitted_model.params`# (actually, `fitted_model.params.values[0]`)\n",
    "y_hat = fitted_model.fittedvalues\n",
    "df['y-hat (from model)'] = y_hat \n",
    "df['y-hat (from formula)'] = 0.536107332688726+0.0023492341183347265*x\n",
    "df\n",
    "\n",
    "# `fitted_model.fittedvalues` is the same as `fitted_model.predict(df)`\n",
    "# but the latter is more genereal \n",
    "# as it could be used to predict new values based on a different data frame..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888da92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals\n",
    "e = Y - fitted_model.fittedvalues  # df['Shuttlecock Price'] - fitted_model.fittedvalues\n",
    "df['e (Residuals)'] = e\n",
    "# or you can just use `fitted_model.resid`\n",
    "df['e (Residuals) v2'] = fitted_model.resid\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f950d2c",
   "metadata": {},
   "source": [
    "- The **intercept** $\\hat \\beta_0$ is `0.5361`\n",
    "- The **slope** $\\hat \\beta_1$  is `0.0023` and is labeled `Q(\"Bird Flu Cases\")` in the output\n",
    "    - The **outcome** $Y_i$ is `Q(\"Shuttlecock Price\")`\n",
    "    - The **predictor** $x_i$ is `Q(\"Bird Flu Cases\")`\n",
    "    - The **slope** is the \"on average\" change in $Y_i$ per \"single unit\" change in $x_i$\n",
    "   \n",
    "- A **fitted (predicted) value** $\\hat y_i$ is found by \"plugging in\" $x_i$ and calculating $0.5361+0.0023 \\times x_i$\n",
    "- A **residual** is calculated as $\\text{e}_i = \\hat \\epsilon_i = y_i - \\hat y_i = y_i - \\hat \\beta_0 + \\hat \\beta_1 x_i $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc031789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is commented to indicate its visualization/demonstration purpose:\n",
    "# students may study smaller details beyond the \"big picture\" later \n",
    "# in a ChatBot session if so inclined\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Here's the model fit visually\n",
    "df['Original Data'] = 'Original Data' # hack to add legend item for data points\n",
    "fig = px.scatter(df, x='Bird Flu Cases',  y='Shuttlecock Price', color='Original Data',\n",
    "                 title='Shuttlecock Price vs. Bird Flu Cases',\n",
    "                 labels={'Bird Flu Cases': 'Bird Flu Cases',\n",
    "                         'Shuttlecock Price': 'Shuttlecock Price ($)'},\n",
    "                 trendline='ols')\n",
    "fig.update_traces(marker=dict(size=10))\n",
    "              \n",
    "# This is what `trendline='ols'` does\n",
    "fig.add_scatter(x=df['Bird Flu Cases'], y=fitted_model.fittedvalues,\n",
    "                line=dict(color='blue', width=3), name=\"trendline='ols'\")\n",
    "    \n",
    "# Adding the line of the math expression\n",
    "x_range = np.array([df['Bird Flu Cases'].min(), df['Bird Flu Cases'].max()])\n",
    "y_line = 0.536107332688726+0.0023492341183347265 * x_range\n",
    "fig.add_scatter(x=x_range, y=y_line, mode='lines', name='0.5361 + 0.0023 * x', \n",
    "                line=dict(dash='dot', color='orange'))\n",
    "\n",
    "# Adding predicted values as points\n",
    "fig.add_scatter(x=df['Bird Flu Cases'], y=df['y-hat (from model)'], mode='markers', \n",
    "                name='Fitted (Predicted) Values', \n",
    "                marker=dict(color='black', symbol='cross', size=10))\n",
    "\n",
    "fig.update_layout(legend_title=None)\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62f30f",
   "metadata": {},
   "source": [
    "### [Omitted] Model Diagnostics: evaluating the assumptions of Simple Linear Regression [0 of the 45 minutes]\n",
    "\n",
    "\n",
    "#### To evaluate if the assumptions of \"normality\" and \"heteroskedasticity\" ($x_i$ agnostic variance) of the theoretical distribution of the error (noise) terms we see if the residuals appear to be normally distributed... this is not really enough data to determine this convincingly one way or another at this point\n",
    "\n",
    "#### In the context of Simple Linear Regression (as opposed to Multiple Linear Regression), we could examine the scatter plot to see if the assumption of a \"linear form of the model\" appears \"true\" plot... in the original scatter plot of the data there appears to be some potential \"curve\" in the relationship, but again there's really not enough data to determine this convincingly \"by eye\" at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d963f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure for demonstration/visualization purposes only:\n",
    "# students may study smaller details later in a ChatBot session if so inclined\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "n = len(df['e (Residuals)'])\n",
    "normality_heteroskedasticity_diagnostic_judgement = \\\n",
    "'<br>[Seems to plausibly be a (n='+str(n)+') sample from a \"normal\" distribution]'\n",
    "df['Observed Residuals'] = 'Observed Residuals' # hack to add legend item for data points\n",
    "fig = px.histogram(df, x='e (Residuals)', color='Observed Residuals',\n",
    "                   title='Histogram of Residuals'+normality_heteroskedasticity_diagnostic_judgement)\n",
    "\n",
    "# rerun this cell to see repeated examples\n",
    "random_normal_sample = stats.norm(loc=0, scale=df['e (Residuals)'].std()).rvs(size=n) \n",
    "\n",
    "fig.add_histogram(x=random_normal_sample, name='Random Normal Sample', \n",
    "                  histfunc='count', opacity=0.5, marker=dict(color='orange'))\n",
    "fig.update_layout(barmode='overlay', legend_title=None)\n",
    "\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89240110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure for demonstration/visualization purposes only:\n",
    "# students may study smaller details later in a ChatBot session if so inclined\n",
    "\n",
    "linearity_diagnostic_judgement = '\"<br>[Straight line\" fit appears \"reasonable\"]'\n",
    "# uncomment/comment `trendline='ols'` to toggle the \"straight line fit\" on and off\n",
    "fig = px.scatter(df, x='Bird Flu Cases',  y='Shuttlecock Price', color='Original Data',\n",
    "                 #trendline='ols',\n",
    "                 title='Shuttlecock Price vs. Bird Flu Cases'+linearity_diagnostic_judgement,\n",
    "                 labels={'Bird Flu Cases': 'Bird Flu Cases',\n",
    "                         'Shuttlecock Price': 'Shuttlecock Price ($)'})\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b3c1c",
   "metadata": {},
   "source": [
    "### Hypothesis Testing (for Simple Linear Regression) [12 of the 45 minutes]\n",
    "\n",
    "We can use **Simple Linear Regression** to test\n",
    "\n",
    "$\\large\n",
    "\\begin{align}\n",
    "H_0: {}& \\beta_1=0 \\quad \\text{ (there is no linear assocation between $Y_i$ and $x_i$ \"on average\")}\\\\\n",
    "H_A: {}& H_0 \\text{ is false}\n",
    "\\end{align}$\n",
    "\n",
    "That is, we can assess the evidence of a linear association in the data based on a **null hypothesis** that the **slope** (the \"on average\" change in $Y_i$ per \"single unit\" change in $x_i$) is zero\n",
    "\n",
    "#### Did your group get $H_0$ correct in your answers for *Communication Activity #1 question 2*?<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> We are essentially never (or only very rarely in very special circumstances) interested in a **null hypothesis** concerning the **intercept** $\\beta_0$ (as opposed to $\\beta_1$)\n",
    "> \n",
    "> $\\Large\n",
    "\\begin{align}\n",
    "H_0: {}& \\beta_0=0\\\\\n",
    "H_A: {}& H_0 \\text{ is false}\n",
    "\\end{align}$\n",
    ">\n",
    "> This is because the assumption that $\\beta_0$ is zero essentially never (or only very rarely in very special circumstances) has any meaning, whereas the assumption that $\\beta_1$ is zero has the very practically useful interpretation of \"no linear association\" which allows us to evaluate the  evidence of a linear association based on observed data\n",
    "    \n",
    "</details>\n",
    "\n",
    "#### How do we use the fitted Simple Linear Regression model to assess $H_0$ regarding $\\beta_1$? Where do we find the p-value we use to make our assessment of $H_0$ and how do we interpret the p-value to make a decision?<br>\n",
    " \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    " \n",
    "Remember, the **p-value** is \"the probability that a test statistic is as or more extreme than the observed test statistic if the null hypothesis is true\"\n",
    "\n",
    "- We do not prove $H_0$ false, we instead give evidence against the $H_0$\n",
    "     - \"We reject the null hypothesis with a p-value of abc, meaning we have xyz evidence against the null hypothesis\"\n",
    "- We do not prove $H_0$ is true, we instead do not have evidence to reject $H_0$\n",
    "     - \"We fail to reject the null hypothesis with a p-value of abc\"\n",
    "|p-value|Evidence|\n",
    "|-|-|\n",
    "|$$p > 0.1$$|No evidence against the null hypothesis|\n",
    "|$$0.1 \\ge p > 0.05$$|Weak evidence against the null hypothesis|\n",
    "|$$0.05 \\ge p > 0.01$$|Moderate evidence against the null hypothesis|\n",
    "|$$0.01 \\ge p > 0.001$$|Strong evidence against the null hypothesis|\n",
    "|$$0.001 \\ge p$$|Very strong evidence against the null hypothesis|\n",
    "   \n",
    "</details>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92857f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.summary().tables[1] # does this indexing make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a4930",
   "metadata": {},
   "source": [
    "## üì¢ üëÇ Communication Activity #2 [final 15 minutes]\n",
    "\n",
    "### If you don't complete this NOW in TUT students will have to complete it ON THEIR OWN for the <u>EXTREMELY VERY BIG AND VERY DIFFERENT</u> Homework 06 <sub>*HW06_Week07ate09_DueNov07STA130_HW06_Week07ate09_DueNov07STA130_HW06_Week07ate09_DueNov07STA130*</sub>\n",
    "\n",
    "In order to follow up and explain the answers to **Communication Activity #1 question 2**, each of the <u>**FIVE**</u> groups from **Communication Activity #1** will sequentially volunteer to present answers to these questions (**taking average three minutes per group**) to the class (in order as quickly as possible, with groups dynamically helping each other answer their question if needed) \n",
    "\n",
    "1. Explain how the \"uncertainty band\" in the `seaborn.regplot` of the **Further Illustrations** below represents a **bootstrapped sampling distribution** (or slighly more accurately something like a \"95% confidence interval\") for \"lines of best fit\"  \n",
    "\n",
    "\n",
    "2. Explain how this so-called \"**sampling distribution**\" of the \"line of best fit\" could be \"sampled\"\n",
    "    1. by making a **bootstrapped sampling distribution** \n",
    "    2. by assuming the **population model** and creating **simulations** \n",
    "\n",
    "\n",
    "3. Explain how the **sampling distribution** of (just) the **estimated slope** $\\hat \\beta_1$ could be **simulated** and the **p-value** for a **null hypothesis** of \"no linear association\" created using **simulation** and used to assess the evidence against the **null hypothesis**  \n",
    "    1. Also explain how a **95% bootstrapped confidence interval** of the **slope coefficient** $\\hat \\beta_1$ could be constructed\n",
    "\n",
    "\n",
    "4. Find the \"R-squared\" in the `fitted_model.summary()` table (or accessible via `fitted_model.rsquared`) and compare this value with \n",
    "    1. `np.corrcoef(Y,x)[0,1]**2`,  \n",
    "    2. `np.corrcoef(Y,y_hat)[0,1]**2`,   \n",
    "    3. and `1-((Y-y_hat)**2).sum()/((Y-Y.mean())**2).sum()` (where `Y`,`x`, and `y_hat` have been defined in the notebook above for the orignal data); \n",
    "    4. then, explain (a) what the two `np.corrcoef...` expressions capture, (b) why the final expression can be interpreted as \"the proportion of variation in (outcome) Y explained by the model (y_hat)\", and (c) therefore why `fitted_model.rsquared` can be interpreted as a measure of the accuracy of the model  \n",
    "\n",
    "\n",
    "5. Explain what our likely judgement about the **Model Diagnostics** in the <u>**Omitted**</u> section above would be for data **simulated** based on an assumed **population model** (as opposed to using **bootstrapping**), and what would cause an analogous judgement them to fail for observed data (and what this means about the appropriateness of the theoretical **Simple Linear Regression** model for this data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c82b6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further Illustration \n",
    "\n",
    "> The `seaborn` plotting library has a function which shows the uncertainty of the (trendline) \"straight line fit\" based on the available data: anywhere a straight line can be drawn through the \"uncertainty band\" is plausible as far as the evidence in the observed data is concerned\n",
    "\n",
    "#### How does the vertical spread of the $Y_i$ outcomes (tightness around the \"line of best fit\") affect the evidence against the null hypothesis? What does this mean in terms of our belief about the evidence against a *null hypothesis* of no linear association \"on average\" between Bird Flu Cases and Shuttlecock Price? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdd59f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Figure for demonstration/visualization purposes only:\n",
    "# students may study smaller details later in a ChatBot session if so inclined\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spread = 20 # increase this to vertically spread the data (1 recreated the original data)\n",
    "df[\"Synthetically Spread y\"] = fitted_model.fittedvalues + spread*df['e (Residuals)']\n",
    "\n",
    "linear_specification_ = 'Q(\"Synthetically Spread y\") ~ Q(\"Bird Flu Cases\")'\n",
    "model_data_specification_ = smf.ols(linear_specification_, data=df)\n",
    "fitted_model_ = model_data_specification_.fit()\n",
    "print(fitted_model_.summary().tables[1])\n",
    "\n",
    "sns.regplot(x='Bird Flu Cases', y='Synthetically Spread y', data=df) #, line_kws={'color': 'red'})\n",
    "plt.title('Bird Flu Cases vs. Shuttlecock Price ($)')\n",
    "plt.xlabel('Bird Flu Cases')\n",
    "plt.ylabel('Shuttlecock Price ($)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b0b792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
